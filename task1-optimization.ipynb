{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5a8lAWOfmmy"
   },
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_bepwPRfmnM"
   },
   "source": [
    "## Assignment 2 - Task 1: Optimization\n",
    "\n",
    "In this task, we introduce several improved stochastic gradient descent (SGD) based optimization methods. Plain/naive SGD is a reasonable method to update neural network parameters. However, to make SGD perform well, one would need to find an appropriate learning rate and a good initial value. Otherwise, the network will get stuck if the learning rate is small, or it will diverge if the learning rate is too large. In reality, since we have no prior knowledge about the training data, it is not trivial to find a good learning rate manually. Also, when the network becomes deeper, for each layer one may need to set a different learning rate. Another common problem is the lack of sufficient training data. This can cause the training to get stuck when using the naive SGD method. These are the limitations of the plain SGD, which are motivators for creating and using improved SGD-based methods. \n",
    "\n",
    "To understand the process of setting a good learning rate, one can rely on adaptive learning rate methods. Here, you are going to experiment with **SGD with momentum**, **SGD with nesterov momentum**, **AdaGrad**, **Adam** and compare them against one another.\n",
    "All of these optimizers are adaptive learning rate methods. Here is a useful link to learn more about each method used in this task: http://ruder.io/optimizing-gradient-descent/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kwQ5NoOfmnP"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da7zcFQffmnd"
   },
   "source": [
    "## Load Fashion-MNIST\n",
    "\n",
    "Here we use a small dataset with only 2500 samples to simulate the \"lack-of-data\" situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1698,
     "status": "ok",
     "timestamp": 1631138412954,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "PpxLY-MWfmne",
    "outputId": "dc0a48ce-34f5-41bb-be66-23a1fb1389bc"
   },
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, val = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_val_raw, y_val = val\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_val = X_val_raw.reshape((X_val_raw.shape[0], X_val_raw.shape[1]**2))\n",
    "\n",
    "#Consider a subset of 2500 samples of the 60000 total images (indexed 10000 ~ 12500)\n",
    "X_val = X_train[10000:10500,:]\n",
    "y_val = y_train[10000:10500]\n",
    "X_train = X_train[10500:12500,:]\n",
    "y_train = y_train[10500:12500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We have vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20tnMCjyfmnh"
   },
   "source": [
    "## Part 1: Implement Several Optimizers\n",
    "\n",
    "Instructors provide code snippets for testing student code implementations.\n",
    "\n",
    "The best anticipated achievable accuracies are around 0.3-0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loz33g8Ufmnk"
   },
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_4iVMT2fmnm"
   },
   "source": [
    "### Original SGD with learning rate decay (for comparison purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4609,
     "status": "ok",
     "timestamp": 1631138989515,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "crxdrUTvfmnn",
    "outputId": "8c1cb517-e687-407d-bcf3-300b29383430"
   },
   "outputs": [],
   "source": [
    "from utils.optimizers import SGDOptim\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 100], num_classes=10, weight_scale=1e-3, l2_reg=0.0)\n",
    "optimizer = SGDOptim()\n",
    "hist_sgd = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=30, batch_size=200, learning_rate=1e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZauLVUBlfmno"
   },
   "source": [
    "### SGD + Momentum\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Implement SGD + Momentum by editing **SGDmomentumOptim** in __./utils/optimizers.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5250,
     "status": "ok",
     "timestamp": 1631139111767,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "oFTM2qtafmno",
    "outputId": "03fbd2a0-336b-4177-98dc-38cb6c5a3027"
   },
   "outputs": [],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import SGDmomentumOptim\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3, momentum=0.8)\n",
    "optimizer = SGDmomentumOptim(model, momentum=0.8)\n",
    "hist_sgd_momentum = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                                         num_epoch=30, batch_size=200, learning_rate=1e-2, \n",
    "                                         learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69CIXBpLfmnq"
   },
   "source": [
    "### AdaGrad\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Implement Adam by editing **AdaGradOptim** in ./utils/optimizers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4608,
     "status": "ok",
     "timestamp": 1631139255047,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "_bZaCA7Qfmnr",
    "outputId": "e740e6d2-d04a-4954-c957-af46c01c387d"
   },
   "outputs": [],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import AdaGradOptim\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = AdaGradOptim(model)\n",
    "hist_adagrad = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                               num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                               learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5ZEf-udfmnv"
   },
   "source": [
    "### Adam\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Implement Adam by editing **AdamOptim** in ./utils/optimizers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5220,
     "status": "ok",
     "timestamp": 1631139271319,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "aTws4phjfmnw",
    "outputId": "90f0732d-6062-4b8e-ffaf-6978d05958bf"
   },
   "outputs": [],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import AdamOptim\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_adam = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                            num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                            learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Implement Nadam by editing **NadamOptim** in __./utils/optimizers.py__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import NadamOptim\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = NadamOptim(model, beta1=0.9, beta2=0.999, eps=1e-8)\n",
    "hist_sgd_nadam = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                                         num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                                         learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dga2dgGxfmny"
   },
   "source": [
    "## Part 2: Comparison\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Run the following cells, which plot the loss, training accuracy, and validation accuracy curves of different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mzui75QUfmny"
   },
   "outputs": [],
   "source": [
    "loss_hist_sgd, train_acc_hist_sgd, val_acc_hist_sgd = hist_sgd\n",
    "loss_hist_momentum, train_acc_hist_momentum, val_acc_hist_momentum = hist_sgd_momentum\n",
    "loss_hist_nesterov, train_acc_hist_nesterov, val_acc_hist_nesterov = hist_sgd_nadam\n",
    "loss_hist_adagrad, train_acc_hist_adagrad, val_acc_hist_adagrad = hist_adagrad\n",
    "loss_hist_adam, train_acc_hist_adam, val_acc_hist_adam = hist_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1631139318716,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "oX9CyZjLfmnz",
    "outputId": "ce4a99cd-8663-4a2c-cdca-6ab7b95125c9"
   },
   "outputs": [],
   "source": [
    "# Plot the training error curves for implemented optimizers\n",
    "plt.plot(loss_hist_sgd, label=\"sgd\")\n",
    "plt.plot(loss_hist_momentum, label=\"momentum\")\n",
    "plt.plot(loss_hist_nesterov, label=\"nadam\")\n",
    "plt.plot(loss_hist_adagrad, label=\"adagrad\")\n",
    "plt.plot(loss_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1631139328348,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "JVigWVU1fmn0",
    "outputId": "c836fcfb-5350-4fbc-ccee-32270aa834e8"
   },
   "outputs": [],
   "source": [
    "# Plot the training accuracy curves for implemented optimizers\n",
    "plt.plot(train_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(train_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(train_acc_hist_nesterov, label=\"nadam\")\n",
    "plt.plot(train_acc_hist_adagrad, label=\"adagrad\")\n",
    "plt.plot(train_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1631139339586,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "7AIf6zjpfmn1",
    "outputId": "25c10de5-13a4-40f9-e721-68f2d9ceb852"
   },
   "outputs": [],
   "source": [
    "# Plot the validation accuracy curves for implemented optimizers\n",
    "plt.plot(val_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(val_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(val_acc_hist_nesterov, label=\"nadam\")\n",
    "plt.plot(val_acc_hist_adagrad, label=\"adagrad\")\n",
    "plt.plot(val_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4aY4q4wfmn2"
   },
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Describe your results, and discuss your understandings of these optimizers, such as their advantages/disadvantages and when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL5H31uzfmn3"
   },
   "source": [
    "Answer: **[fill in here]**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1-optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
